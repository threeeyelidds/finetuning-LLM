Using custom data configuration default-74b8cbaeec9bed04
Loading Dataset Infos from /home/quantinx/.local/lib/python3.9/site-packages/datasets/packaged_modules/json
Generating dataset json (/home/quantinx/.cache/huggingface/datasets/json/default-74b8cbaeec9bed04/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)
Downloading and preparing dataset json/default to /home/quantinx/.cache/huggingface/datasets/json/default-74b8cbaeec9bed04/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...
Downloading took 0.0 min
Checksum Computation took 0.0 min
Generating train split
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 22434 examples [00:00, 890258.82 examples/s]
Unable to verify splits sizes.
Dataset json downloaded and prepared to /home/quantinx/.cache/huggingface/datasets/json/default-74b8cbaeec9bed04/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.
--- Logging error ---
Traceback (most recent call last):
  File "/usr/lib64/python3.9/logging/__init__.py", line 1083, in emit
    msg = self.format(record)
  File "/usr/lib64/python3.9/logging/__init__.py", line 927, in format
    return fmt.format(record)
  File "/usr/lib64/python3.9/logging/__init__.py", line 663, in format
    record.message = record.getMessage()
  File "/usr/lib64/python3.9/logging/__init__.py", line 367, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Call stack:
  File "/home/quantinx/finetuning-LLM/src/run_sft.py", line 230, in <module>
    main()
  File "/home/quantinx/finetuning-LLM/src/run_sft.py", line 115, in main
    logger.info("dataset length",len(train_dataset))
Message: 'dataset length'
Arguments: (22434,)
[INFO|tokenization_utils_base.py:2027] 2024-02-08 12:20:26,810 >> loading file vocab.json from cache at /home/quantinx/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/b10c3eba545ad279e7208ee3a5d644566f001670/vocab.json
[INFO|tokenization_utils_base.py:2027] 2024-02-08 12:20:26,810 >> loading file merges.txt from cache at /home/quantinx/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/b10c3eba545ad279e7208ee3a5d644566f001670/merges.txt
[INFO|tokenization_utils_base.py:2027] 2024-02-08 12:20:26,810 >> loading file tokenizer.json from cache at /home/quantinx/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/b10c3eba545ad279e7208ee3a5d644566f001670/tokenizer.json
[INFO|tokenization_utils_base.py:2027] 2024-02-08 12:20:26,810 >> loading file added_tokens.json from cache at /home/quantinx/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/b10c3eba545ad279e7208ee3a5d644566f001670/added_tokens.json
[INFO|tokenization_utils_base.py:2027] 2024-02-08 12:20:26,810 >> loading file special_tokens_map.json from cache at /home/quantinx/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/b10c3eba545ad279e7208ee3a5d644566f001670/special_tokens_map.json
[INFO|tokenization_utils_base.py:2027] 2024-02-08 12:20:26,810 >> loading file tokenizer_config.json from cache at /home/quantinx/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/b10c3eba545ad279e7208ee3a5d644566f001670/tokenizer_config.json
[WARNING|logging.py:314] 2024-02-08 12:20:26,857 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[WARNING|logging.py:329] 2024-02-08 12:20:26,858 >> 
No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format (without BOS/EOS tokens!). If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.

/home/quantinx/.local/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You passed a model_id to the SFTTrainer. This will automatically create an `AutoModelForCausalLM` or a `PeftModel` (if you passed a `peft_config`) for you.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-02-08 12:20:26,911 >> loading configuration file config.json from cache at /home/quantinx/models/models--microsoft--phi-2/snapshots/b10c3eba545ad279e7208ee3a5d644566f001670/config.json
[INFO|configuration_utils.py:792] 2024-02-08 12:20:26,911 >> Model config PhiConfig {
  "_name_or_path": "microsoft/phi-2",
  "architectures": [
    "PhiForCausalLM"
  ],
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "microsoft/phi-2--configuration_phi.PhiConfig",
    "AutoModelForCausalLM": "microsoft/phi-2--modeling_phi.PhiForCausalLM"
  },
  "bos_token_id": 50256,
  "embd_pdrop": 0.0,
  "eos_token_id": 50256,
  "hidden_act": "gelu_new",
  "hidden_size": 2560,
  "initializer_range": 0.02,
  "intermediate_size": 10240,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "phi",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "partial_rotary_factor": 0.4,
  "qk_layernorm": false,
  "resid_pdrop": 0.1,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.37.2",
  "use_cache": false,
  "vocab_size": 51200
}

[INFO|modeling_utils.py:3476] 2024-02-08 12:20:26,917 >> loading weights file model.safetensors from cache at /home/quantinx/models/models--microsoft--phi-2/snapshots/b10c3eba545ad279e7208ee3a5d644566f001670/model.safetensors.index.json
[INFO|modeling_utils.py:3539] 2024-02-08 12:20:26,919 >> Will use torch_dtype=torch.float16 as defined in model's config object
[INFO|modeling_utils.py:1426] 2024-02-08 12:20:26,919 >> Instantiating PhiForCausalLM model under default dtype torch.float16.
[INFO|configuration_utils.py:826] 2024-02-08 12:20:26,920 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256,
  "use_cache": false
}

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.17s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.66s/it]
[INFO|modeling_utils.py:4350] 2024-02-08 12:20:34,607 >> All model checkpoint weights were used when initializing PhiForCausalLM.

[INFO|modeling_utils.py:4358] 2024-02-08 12:20:34,607 >> All the weights of PhiForCausalLM were initialized from the model checkpoint at microsoft/phi-2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use PhiForCausalLM for predictions without further training.
[INFO|configuration_utils.py:781] 2024-02-08 12:20:34,804 >> loading configuration file generation_config.json from cache at /home/quantinx/models/models--microsoft--phi-2/snapshots/b10c3eba545ad279e7208ee3a5d644566f001670/generation_config.json
[INFO|configuration_utils.py:826] 2024-02-08 12:20:34,804 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

/home/quantinx/.local/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024
  warnings.warn(
Map:   0%|          | 0/22434 [00:00<?, ? examples/s]Caching processed dataset at /home/quantinx/.cache/huggingface/datasets/json/default-74b8cbaeec9bed04/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-c614921fb1533f35.arrow
Map:   9%|▉         | 2000/22434 [00:00<00:01, 12538.89 examples/s]Map:  18%|█▊        | 4000/22434 [00:00<00:01, 13370.97 examples/s]Map:  27%|██▋       | 6000/22434 [00:00<00:01, 13632.27 examples/s]Map:  36%|███▌      | 8000/22434 [00:00<00:01, 13946.90 examples/s]Map:  45%|████▍     | 10000/22434 [00:00<00:00, 13990.10 examples/s]Map:  53%|█████▎    | 12000/22434 [00:00<00:00, 14144.38 examples/s]Map:  62%|██████▏   | 14000/22434 [00:01<00:00, 14259.09 examples/s]Map:  71%|███████▏  | 16000/22434 [00:01<00:00, 14223.44 examples/s]Map:  80%|████████  | 18000/22434 [00:01<00:00, 14239.90 examples/s]Map:  89%|████████▉ | 20000/22434 [00:01<00:00, 14230.71 examples/s]Map:  98%|█████████▊| 22000/22434 [00:01<00:00, 14259.94 examples/s]Map: 100%|██████████| 22434/22434 [00:01<00:00, 13948.51 examples/s]
[INFO|trainer.py:1721] 2024-02-08 12:20:37,086 >> ***** Running training *****
[INFO|trainer.py:1722] 2024-02-08 12:20:37,086 >>   Num examples = 22,434
[INFO|trainer.py:1723] 2024-02-08 12:20:37,086 >>   Num Epochs = 3
[INFO|trainer.py:1724] 2024-02-08 12:20:37,086 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:1727] 2024-02-08 12:20:37,086 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1728] 2024-02-08 12:20:37,086 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:1729] 2024-02-08 12:20:37,086 >>   Total optimization steps = 4,206
[INFO|trainer.py:1730] 2024-02-08 12:20:37,088 >>   Number of trainable parameters = 31,457,280
  0%|          | 0/4206 [00:00<?, ?it/s]  0%|          | 1/4206 [00:01<1:38:14,  1.40s/it]                                                    0%|          | 1/4206 [00:01<1:38:14,  1.40s/it]  0%|          | 2/4206 [00:02<1:12:44,  1.04s/it]  0%|          | 3/4206 [00:02<1:04:17,  1.09it/s]  0%|          | 4/4206 [00:03<1:00:10,  1.16it/s]  0%|          | 5/4206 [00:04<58:11,  1.20it/s]                                                    0%|          | 5/4206 [00:04<58:11,  1.20it/s]  0%|          | 6/4206 [00:05<59:43,  1.17it/s]  0%|          | 7/4206 [00:06<57:58,  1.21it/s]  0%|          | 8/4206 [00:06<57:25,  1.22it/s]  0%|          | 9/4206 [00:07<56:23,  1.24it/s]  0%|          | 10/4206 [00:08<55:45,  1.25it/s]                                                   0%|          | 10/4206 [00:08<55:45,  1.25it/s]  0%|          | 11/4206 [00:09<55:27,  1.26it/s]  0%|          | 12/4206 [00:10<57:58,  1.21it/s]  0%|          | 13/4206 [00:11<56:59,  1.23it/s]  0%|          | 14/4206 [00:11<56:40,  1.23it/s]  0%|          | 15/4206 [00:12<56:38,  1.23it/s]                                                   0%|          | 15/4206 [00:12<56:38,  1.23it/s]  0%|          | 16/4206 [00:13<56:06,  1.24it/s]  0%|          | 17/4206 [00:14<59:42,  1.17it/s]  0%|          | 18/4206 [00:15<59:27,  1.17it/s]  0%|          | 19/4206 [00:16<57:54,  1.21it/s]  0%|          | 20/4206 [00:16<56:58,  1.22it/s]                                                   0%|          | 20/4206 [00:16<56:58,  1.22it/s]  0%|          | 21/4206 [00:17<57:11,  1.22it/s]  1%|          | 22/4206 [00:18<56:39,  1.23it/s]  1%|          | 23/4206 [00:19<59:12,  1.18it/s]  1%|          | 24/4206 [00:20<57:50,  1.21it/s]  1%|          | 25/4206 [00:20<56:58,  1.22it/s]                                                   1%|          | 25/4206 [00:20<56:58,  1.22it/s]  1%|          | 26/4206 [00:21<56:34,  1.23it/s]  1%|          | 27/4206 [00:22<56:13,  1.24it/s]  1%|          | 28/4206 [00:23<56:10,  1.24it/s]  1%|          | 29/4206 [00:24<58:53,  1.18it/s]  1%|          | 30/4206 [00:25<57:34,  1.21it/s]                                                   1%|          | 30/4206 [00:25<57:34,  1.21it/s]  1%|          | 31/4206 [00:25<56:48,  1.22it/s]  1%|          | 32/4206 [00:26<56:02,  1.24it/s]  1%|          | 33/4206 [00:27<55:39,  1.25it/s]  1%|          | 34/4206 [00:28<58:18,  1.19it/s]  1%|          | 35/4206 [00:29<57:39,  1.21it/s]                                                   1%|          | 35/4206 [00:29<57:39,  1.21it/s]  1%|          | 36/4206 [00:29<56:51,  1.22it/s]  1%|          | 37/4206 [00:30<56:17,  1.23it/s]  1%|          | 38/4206 [00:31<55:39,  1.25it/s]  1%|          | 39/4206 [00:32<55:19,  1.26it/s]  1%|          | 40/4206 [00:33<57:23,  1.21it/s]                                                   1%|          | 40/4206 [00:33<57:23,  1.21it/s]  1%|          | 41/4206 [00:34<56:58,  1.22it/s]  1%|          | 42/4206 [00:34<56:23,  1.23it/s]  1%|          | 43/4206 [00:35<55:41,  1.25it/s]  1%|          | 44/4206 [00:36<55:13,  1.26it/s]  1%|          | 45/4206 [00:37<57:34,  1.20it/s]                                                   1%|          | 45/4206 [00:37<57:34,  1.20it/s]  1%|          | 46/4206 [00:38<56:31,  1.23it/s]  1%|          | 47/4206 [00:38<56:09,  1.23it/s]  1%|          | 48/4206 [00:39<55:32,  1.25it/s]  1%|          | 49/4206 [00:40<55:02,  1.26it/s]  1%|          | 50/4206 [00:41<54:48,  1.26it/s]                                                   1%|          | 50/4206 [00:41<54:48,  1.26it/s][INFO|trainer.py:2936] 2024-02-08 12:21:18,285 >> Saving model checkpoint to data/phi-sft-lora/tmp-checkpoint-50
[INFO|configuration_utils.py:729] 2024-02-08 12:21:18,425 >> loading configuration file config.json from cache at /home/quantinx/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/b10c3eba545ad279e7208ee3a5d644566f001670/config.json
[INFO|configuration_utils.py:792] 2024-02-08 12:21:18,426 >> Model config PhiConfig {
  "_name_or_path": "microsoft/phi-2",
  "architectures": [
    "PhiForCausalLM"
  ],
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "microsoft/phi-2--configuration_phi.PhiConfig",
    "AutoModelForCausalLM": "microsoft/phi-2--modeling_phi.PhiForCausalLM"
  },
  "bos_token_id": 50256,
  "embd_pdrop": 0.0,
  "eos_token_id": 50256,
  "hidden_act": "gelu_new",
  "hidden_size": 2560,
  "initializer_range": 0.02,
  "intermediate_size": 10240,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "phi",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "partial_rotary_factor": 0.4,
  "qk_layernorm": false,
  "resid_pdrop": 0.1,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 51200
}

[INFO|tokenization_utils_base.py:2433] 2024-02-08 12:21:18,535 >> tokenizer config file saved in data/phi-sft-lora/tmp-checkpoint-50/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-02-08 12:21:18,535 >> Special tokens file saved in data/phi-sft-lora/tmp-checkpoint-50/special_tokens_map.json
  1%|          | 51/4206 [00:42<1:07:34,  1.02it/s]  1%|          | 52/4206 [00:43<1:03:29,  1.09it/s]  1%|▏         | 53/4206 [00:44<1:01:27,  1.13it/s]  1%|▏         | 54/4206 [00:44<59:19,  1.17it/s]    1%|▏         | 55/4206 [00:45<57:41,  1.20it/s]                                                   1%|▏         | 55/4206 [00:45<57:41,  1.20it/s]  1%|▏         | 56/4206 [00:46<59:33,  1.16it/s]  1%|▏         | 57/4206 [00:47<58:45,  1.18it/s]  1%|▏         | 58/4206 [00:48<57:47,  1.20it/s]  1%|▏         | 59/4206 [00:49<56:36,  1.22it/s]  1%|▏         | 60/4206 [00:49<56:07,  1.23it/s]                                                   1%|▏         | 60/4206 [00:49<56:07,  1.23it/s]  1%|▏         | 61/4206 [00:50<55:26,  1.25it/s]  1%|▏         | 62/4206 [00:51<57:13,  1.21it/s]  1%|▏         | 63/4206 [00:52<56:23,  1.22it/s]  2%|▏         | 64/4206 [00:53<56:08,  1.23it/s]  2%|▏         | 65/4206 [00:53<55:48,  1.24it/s]                                                   2%|▏         | 65/4206 [00:53<55:48,  1.24it/s]  2%|▏         | 66/4206 [00:54<56:24,  1.22it/s]  2%|▏         | 67/4206 [00:55<55:42,  1.24it/s]  2%|▏         | 68/4206 [00:56<57:56,  1.19it/s]  2%|▏         | 69/4206 [00:57<56:35,  1.22it/s]  2%|▏         | 70/4206 [00:58<55:50,  1.23it/s]                                                   2%|▏         | 70/4206 [00:58<55:50,  1.23it/s]  2%|▏         | 71/4206 [00:58<55:19,  1.25it/s]  2%|▏         | 72/4206 [00:59<55:06,  1.25it/s]  2%|▏         | 73/4206 [01:00<58:02,  1.19it/s]  2%|▏         | 74/4206 [01:01<56:40,  1.22it/s]  2%|▏         | 75/4206 [01:02<55:44,  1.24it/s]                                                   2%|▏         | 75/4206 [01:02<55:44,  1.24it/s]  2%|▏         | 76/4206 [01:02<55:12,  1.25it/s]  2%|▏         | 77/4206 [01:03<54:49,  1.26it/s]  2%|▏         | 78/4206 [01:04<54:32,  1.26it/s]  2%|▏         | 79/4206 [01:05<56:51,  1.21it/s]  2%|▏         | 80/4206 [01:06<55:58,  1.23it/s]                                                   2%|▏         | 80/4206 [01:06<55:58,  1.23it/s]  2%|▏         | 81/4206 [01:06<55:22,  1.24it/s]  2%|▏         | 82/4206 [01:07<55:26,  1.24it/s]  2%|▏         | 83/4206 [01:08<54:57,  1.25it/s]  2%|▏         | 84/4206 [01:09<54:29,  1.26it/s]  2%|▏         | 85/4206 [01:10<54:13,  1.27it/s]                                                   2%|▏         | 85/4206 [01:10<54:13,  1.27it/s]  2%|▏         | 86/4206 [01:11<56:41,  1.21it/s]  2%|▏         | 87/4206 [01:11<55:45,  1.23it/s]  2%|▏         | 88/4206 [01:12<55:17,  1.24it/s]  2%|▏         | 89/4206 [01:13<55:53,  1.23it/s]  2%|▏         | 90/4206 [01:14<55:14,  1.24it/s]                                                   2%|▏         | 90/4206 [01:14<55:14,  1.24it/s]  2%|▏         | 91/4206 [01:15<57:01,  1.20it/s]  2%|▏         | 92/4206 [01:15<56:35,  1.21it/s]  2%|▏         | 93/4206 [01:16<55:40,  1.23it/s]  2%|▏         | 94/4206 [01:17<54:53,  1.25it/s]  2%|▏         | 95/4206 [01:18<54:17,  1.26it/s]                                                   2%|▏         | 95/4206 [01:18<54:17,  1.26it/s]  2%|▏         | 96/4206 [01:19<56:51,  1.20it/s]  2%|▏         | 97/4206 [01:19<55:58,  1.22it/s]  2%|▏         | 98/4206 [01:20<55:08,  1.24it/s]  2%|▏         | 99/4206 [01:21<55:00,  1.24it/s]  2%|▏         | 100/4206 [01:22<54:30,  1.26it/s]                                                    2%|▏         | 100/4206 [01:22<54:30,  1.26it/s][INFO|trainer.py:2936] 2024-02-08 12:21:59,395 >> Saving model checkpoint to data/phi-sft-lora/tmp-checkpoint-100
[INFO|configuration_utils.py:729] 2024-02-08 12:21:59,500 >> loading configuration file config.json from cache at /home/quantinx/.cache/huggingface/hub/models--microsoft--phi-2/snapshots/b10c3eba545ad279e7208ee3a5d644566f001670/config.json
[INFO|configuration_utils.py:792] 2024-02-08 12:21:59,500 >> Model config PhiConfig {
  "_name_or_path": "microsoft/phi-2",
  "architectures": [
    "PhiForCausalLM"
  ],
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "microsoft/phi-2--configuration_phi.PhiConfig",
    "AutoModelForCausalLM": "microsoft/phi-2--modeling_phi.PhiForCausalLM"
  },
  "bos_token_id": 50256,
  "embd_pdrop": 0.0,
  "eos_token_id": 50256,
  "hidden_act": "gelu_new",
  "hidden_size": 2560,
  "initializer_range": 0.02,
  "intermediate_size": 10240,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 2048,
  "model_type": "phi",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "partial_rotary_factor": 0.4,
  "qk_layernorm": false,
  "resid_pdrop": 0.1,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 51200
}

[INFO|tokenization_utils_base.py:2433] 2024-02-08 12:21:59,609 >> tokenizer config file saved in data/phi-sft-lora/tmp-checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-02-08 12:21:59,610 >> Special tokens file saved in data/phi-sft-lora/tmp-checkpoint-100/special_tokens_map.json
  2%|▏         | 101/4206 [01:23<1:04:19,  1.06it/s]  2%|▏         | 102/4206 [01:24<1:03:37,  1.08it/s]  2%|▏         | 103/4206 [01:25<1:00:40,  1.13it/s]  2%|▏         | 104/4206 [01:26<58:26,  1.17it/s]    2%|▏         | 105/4206 [01:26<57:15,  1.19it/s]                                                    2%|▏         | 105/4206 [01:26<57:15,  1.19it/s]  3%|▎         | 106/4206 [01:27<56:05,  1.22it/s]  3%|▎         | 107/4206 [01:28<55:13,  1.24it/s]  3%|▎         | 108/4206 [01:29<57:32,  1.19it/s]  3%|▎         | 109/4206 [01:30<57:09,  1.19it/s]  3%|▎         | 110/4206 [01:30<56:39,  1.20it/s]                                                    3%|▎         | 110/4206 [01:30<56:39,  1.20it/s]slurmstepd: error: *** JOB 154584 ON babel-2-12 CANCELLED AT 2024-02-08T12:22:08 ***
